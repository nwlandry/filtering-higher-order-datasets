{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3687bb4b",
   "metadata": {},
   "source": [
    "# Filtering higher-order datasets\n",
    "\n",
    "This notebook accompanies \"Filtering higher-order datasets\" by Nicholas W. Landry, Ilya Amburg, Mirah Shi, and Sinan G. Aksoy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f36063",
   "metadata": {},
   "source": [
    "The following notebook extensively uses the [XGI](https://xgi.readthedocs.io/) software package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddffa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import warnings\n",
    "from scipy.sparse import lil_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import xgi\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.sparse import diags\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from sklearn.cluster import KMeans\n",
    "import json\n",
    "import os\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3cc4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directories if they don't exist\n",
    "if not os.path.exists(\"Data\"):\n",
    "    os.mkdir(\"Data\")\n",
    "if not os.path.exists(\"Figures\"):\n",
    "    os.mkdir(\"Figures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98158cff",
   "metadata": {},
   "source": [
    "### Loading datasets\n",
    "\n",
    "We load all the datasets in this study from the [xgi-data repository](https://gitlab.com/complexgroupinteractions/xgi-data) using a function from [XGI](https://xgi.readthedocs.io/). The algorithms in this notebook may be applied to any dataset, but the following cell handles loading datasets from xgi-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ee16f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "\n",
    "dataset_name = \"email-enron\"\n",
    "dataset_name = \"email-eu\"\n",
    "dataset_name = \"disgenenet\"\n",
    "# dataset_name = \"diseasome\"\n",
    "# dataset_name = \"contact-primary-school\"\n",
    "# dataset_name = \"contact-high-school\"\n",
    "\n",
    "max_order = {\n",
    "    \"email-enron\": 17,\n",
    "    \"email-eu\": 39,\n",
    "    \"disgenenet\": 19,\n",
    "    \"diseasome\": 10,\n",
    "    \"contact-primary-school\": 4,\n",
    "    \"contact-high-school\": 4,\n",
    "}\n",
    "\n",
    "dataset = xgi.load_xgi_data(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0090edfe",
   "metadata": {},
   "source": [
    "### Cleaning the dataset\n",
    "\n",
    "We remove isolated nodes, multiedges, and singletons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf942b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the dual instead of the original hypergraph in some cases.\n",
    "if dataset_name == \"disgenenet\":\n",
    "    dataset = dataset.dual()\n",
    "\n",
    "# This line filters out edge sizes that we wish to exclude from our filtered datasets.\n",
    "# For the email-enron dataset, for example, there is a single edge that is larger than\n",
    "# 18 and it has a size of 37.\n",
    "dataset = xgi.subhypergraph(\n",
    "    dataset, edges=dataset.edges.filterby(\"order\", max_order[dataset_name], \"leq\")\n",
    ").copy()\n",
    "\n",
    "# remove isolates, singletons, and multiedges\n",
    "dataset.cleanup()\n",
    "\n",
    "nodes = list(dataset.nodes)\n",
    "(dataset.num_nodes, dataset.num_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbb5d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we plot the degree and edge size distributions\n",
    "axis_label_fontsize = 18\n",
    "title_fontsize = 16\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "data = dataset.nodes.degree.ashist(log_binning=True, bins=5)\n",
    "plt.loglog(data[\"bin_center\"], data[\"value\"], \"ko\", markersize=2)\n",
    "plt.title(\"Degree distribution\", fontsize=title_fontsize)\n",
    "plt.xlabel(r\"$k$\", fontsize=axis_label_fontsize)\n",
    "plt.ylabel(r\"$P(k)$\", fontsize=axis_label_fontsize)\n",
    "\n",
    "plt.subplot(122)\n",
    "data = dataset.edges.size.ashist(log_binning=True, bins=10)\n",
    "plt.loglog(data[\"bin_center\"], data[\"value\"], \"ko\", markersize=2)\n",
    "plt.title(\"Edge size distribution\", fontsize=title_fontsize)\n",
    "plt.xlabel(r\"$s$\", fontsize=axis_label_fontsize)\n",
    "plt.ylabel(r\"$P(s)$\", fontsize=axis_label_fontsize)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ab42cc",
   "metadata": {},
   "source": [
    "### Generating sets of filterings\n",
    "\n",
    "We create filtering sets (lists of hypergraphs) by only keeping the hyperedges that satisfy the relationship specified in the `filterby()` function. We do this for four different types of filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ef5b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering_parameters = np.arange(\n",
    "    dataset.edges.size.min(), dataset.edges.size.max() + 1, 1, dtype=int\n",
    ")\n",
    "\n",
    "uniform_filtering = [\n",
    "    xgi.subhypergraph(dataset, edges=dataset.edges.filterby(\"size\", k, \"eq\")).copy()\n",
    "    for k in filtering_parameters\n",
    "]\n",
    "geq_filtering = [\n",
    "    xgi.subhypergraph(dataset, edges=dataset.edges.filterby(\"size\", k, \"geq\")).copy()\n",
    "    for k in filtering_parameters\n",
    "]\n",
    "leq_filtering = [\n",
    "    xgi.subhypergraph(dataset, edges=dataset.edges.filterby(\"size\", k, \"leq\")).copy()\n",
    "    for k in filtering_parameters\n",
    "]\n",
    "exclusion_filtering = [\n",
    "    xgi.subhypergraph(dataset, edges=dataset.edges.filterby(\"size\", k, \"neq\")).copy()\n",
    "    for k in filtering_parameters\n",
    "]\n",
    "filterings = {\n",
    "    \"uniform\": uniform_filtering,\n",
    "    \"GEQ\": geq_filtering,\n",
    "    \"LEQ\": leq_filtering,\n",
    "    \"exclusion\": exclusion_filtering,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82377529",
   "metadata": {},
   "source": [
    "### Effective information\n",
    "\n",
    "The following computes the effective information for a filtering set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f4592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_effective_information(filtering):\n",
    "    K = len(filtering)\n",
    "    infos = []\n",
    "    for k in range(K):\n",
    "        print(k)\n",
    "        try:\n",
    "            H = filtering[k]\n",
    "\n",
    "            # This algorithm is the effective information defined\n",
    "            # in \"The Emergence of Informative Higher Scales in Complex Networks\"\n",
    "            # by Klein and Hoel, applied to the weighted clique projection of the hypergraph.\n",
    "            I = xgi.incidence_matrix(H, sparse=True)\n",
    "\n",
    "            # exclude isolated nodes\n",
    "            present_nodes = np.sort(np.unique(I.nonzero()[0]))\n",
    "            I = I[present_nodes, :]\n",
    "\n",
    "            # form the adjacency matrix\n",
    "            A = I.dot(I.T)\n",
    "            n = A.shape[0]\n",
    "            P = lil_matrix((n, n))\n",
    "            d = np.sum(A, axis=0)\n",
    "\n",
    "            P = A / d\n",
    "            Wavg = np.zeros(n)\n",
    "            for i in range(n):\n",
    "                Wavg[i] = np.sum(P[:, i]) / n\n",
    "            first = stats.entropy(Wavg, base=2)\n",
    "            second = (\n",
    "                np.sum(\n",
    "                    [\n",
    "                        stats.entropy([P[i, j] for j in range(n)], base=2)\n",
    "                        for i in range(n)\n",
    "                    ]\n",
    "                )\n",
    "                / n\n",
    "            )\n",
    "            infos.append((first - second) / np.log2(n))\n",
    "        except:\n",
    "            infos.append(np.NaN)\n",
    "    return infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb7a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(f\"Data/{dataset_name}_effective_information.json\") as file:\n",
    "        effective_information = json.loads(file.read())\n",
    "except:\n",
    "    effective_information = dict()\n",
    "    for i, f in enumerate(filterings):\n",
    "        print(\"filtering \", i)\n",
    "        effective_information[f] = get_effective_information(filterings[f])\n",
    "\n",
    "    datastring = json.dumps(effective_information, indent=2)\n",
    "    with open(f\"Data/{dataset_name}_effective_information.json\", \"w\") as output_file:\n",
    "        output_file.write(datastring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecfacf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we plot the effective information for the four different types of filtering.\n",
    "\n",
    "tick_fontsize = 14\n",
    "axis_label_fontsize = 18\n",
    "legend_fontsize = 16\n",
    "title_fontsize = 16\n",
    "xtick_skip = {\n",
    "    \"email-enron\": 3,\n",
    "    \"email-eu\": 10,\n",
    "    \"disgenenet\": 3,\n",
    "    \"diseasome\": 2,\n",
    "    \"contact-primary-school\": 1,\n",
    "    \"contact-high-school\": 1,\n",
    "}\n",
    "\n",
    "c1 = \"#000000\"\n",
    "c2 = \"#648FFF\"\n",
    "c3 = \"#785EF0\"\n",
    "c4 = \"#DC267F\"\n",
    "colors = [c1, c2, c3, c4]\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "specs = {\"uniform\": \"kd-\", \"GEQ\": \"bo-\", \"LEQ\": \"gs-\", \"exclusion\": \"r^-\"}\n",
    "\n",
    "i = 0\n",
    "for label, ei in effective_information.items():\n",
    "    plt.plot(\n",
    "        filtering_parameters,\n",
    "        ei,\n",
    "        specs[label],\n",
    "        label=label,\n",
    "        linewidth=2,\n",
    "        markersize=8,\n",
    "        color=colors[i],\n",
    "    )\n",
    "    i += 1\n",
    "\n",
    "plt.xticks(filtering_parameters[:: xtick_skip[dataset_name]], fontsize=tick_fontsize)\n",
    "plt.yticks(fontsize=tick_fontsize)\n",
    "plt.legend(fontsize=legend_fontsize, loc=\"upper right\")\n",
    "plt.xlabel(r\"$k$\", fontsize=axis_label_fontsize)\n",
    "plt.ylabel(\"EI\", fontsize=axis_label_fontsize)\n",
    "\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "\n",
    "plt.savefig(f\"Figures/{dataset_name}_effective-info.png\", dpi=1000)\n",
    "plt.savefig(f\"Figures/{dataset_name}_effective-info.pdf\", dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eee7a72",
   "metadata": {},
   "source": [
    "### Assortativity\n",
    "\n",
    "Here we compute the degree assortativity according to the definitions in \"Configuration models of random hypergraphs\" by Phil Chodrow and \"Hypergraph assortativity: a dynamical systems perspective\" by Landry and Restrepo. All of these functions are implemented in XGI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e66be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_assortativity(filtering, assortativity_fcn, **kwargs):\n",
    "    a = np.zeros(len(filtering))\n",
    "    for i in range(len(filtering)):\n",
    "        try:\n",
    "            a[i] = assortativity_fcn(filtering[i], **kwargs)\n",
    "        except:\n",
    "            a[i] = np.NaN\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae91501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamical assortativity\n",
    "da = dict()\n",
    "# top-2 assortativity\n",
    "tta = dict()\n",
    "# top-bottom assortativity\n",
    "tba = dict()\n",
    "# uniform assortativity\n",
    "ua = dict()\n",
    "\n",
    "try:\n",
    "    with open(f\"Data/{dataset_name}_dynamical_assortativity.json\") as file:\n",
    "        da = json.loads(file.read())\n",
    "    with open(f\"Data/{dataset_name}_top-2_assortativity.json\") as file:\n",
    "        tta = json.loads(file.read())\n",
    "    with open(f\"Data/{dataset_name}_top-bottom_assortativity.json\") as file:\n",
    "        tba = json.loads(file.read())\n",
    "    with open(f\"Data/{dataset_name}_uniform_assortativity.json\") as file:\n",
    "        ua = json.loads(file.read())\n",
    "except:\n",
    "    for label in filterings:\n",
    "        f = filterings[label]\n",
    "        da[label] = list(get_assortativity(f, xgi.dynamical_assortativity))\n",
    "        tta[label] = list(get_assortativity(f, xgi.degree_assortativity, kind=\"top-2\"))\n",
    "        tba[label] = list(\n",
    "            get_assortativity(f, xgi.degree_assortativity, kind=\"top-bottom\")\n",
    "        )\n",
    "        ua[label] = list(get_assortativity(f, xgi.degree_assortativity, kind=\"uniform\"))\n",
    "\n",
    "    datastring = json.dumps(da, indent=2)\n",
    "    with open(f\"Data/{dataset_name}_dynamical_assortativity.json\", \"w\") as output_file:\n",
    "        output_file.write(datastring)\n",
    "    datastring = json.dumps(tta, indent=2)\n",
    "    with open(f\"Data/{dataset_name}_top-2_assortativity.json\", \"w\") as output_file:\n",
    "        output_file.write(datastring)\n",
    "    datastring = json.dumps(tba, indent=2)\n",
    "    with open(f\"Data/{dataset_name}_top-bottom_assortativity.json\", \"w\") as output_file:\n",
    "        output_file.write(datastring)\n",
    "    datastring = json.dumps(ua, indent=2)\n",
    "    with open(f\"Data/{dataset_name}_uniform_assortativity.json\", \"w\") as output_file:\n",
    "        output_file.write(datastring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5664abee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the assortativity for the four different types of filtering.\n",
    "tick_fontsize = 14\n",
    "axis_label_fontsize = 18\n",
    "legend_fontsize = 12\n",
    "title_fontsize = 16\n",
    "xtick_skip = {\n",
    "    \"email-enron\": 3,\n",
    "    \"email-eu\": 10,\n",
    "    \"disgenenet\": 3,\n",
    "    \"diseasome\": 2,\n",
    "    \"contact-primary-school\": 1,\n",
    "    \"contact-high-school\": 1,\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "c1 = \"#000000\"\n",
    "c2 = \"#648FFF\"\n",
    "c3 = \"#785EF0\"\n",
    "c4 = \"#DC267F\"\n",
    "colors = [c1, c2, c3, c4]\n",
    "\n",
    "for i, label in enumerate(filterings):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.title(f\"{label} filtering\", fontsize=14)\n",
    "    plt.plot(\n",
    "        filtering_parameters, da[label], \"d-\", label=\"dyn.assort.\", color=colors[0]\n",
    "    )\n",
    "    plt.plot(\n",
    "        filtering_parameters, tta[label], \"^-\", label=\"top-2 assort.\", color=colors[1]\n",
    "    )\n",
    "    plt.plot(\n",
    "        filtering_parameters,\n",
    "        tba[label],\n",
    "        \"o-\",\n",
    "        label=\"top-bottom assort.\",\n",
    "        color=colors[2],\n",
    "    )\n",
    "    plt.plot(\n",
    "        filtering_parameters, ua[label], \"s-\", label=\"unif. assort.\", color=colors[3]\n",
    "    )\n",
    "\n",
    "    plt.xticks(\n",
    "        filtering_parameters[:: xtick_skip[dataset_name]], fontsize=tick_fontsize\n",
    "    )\n",
    "    plt.ylim([-1.1, 1.1])\n",
    "    plt.yticks(np.linspace(-1, 1, 5), fontsize=tick_fontsize)\n",
    "    if i == 2:\n",
    "        plt.xlabel(r\"$k$\", fontsize=axis_label_fontsize)\n",
    "        plt.ylabel(r\"$\\rho$\", fontsize=axis_label_fontsize)\n",
    "    if i == 3:\n",
    "        plt.legend(fontsize=legend_fontsize, loc=\"lower right\")\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"Figures/{dataset_name}_assortativity.pdf\", dpi=1000)\n",
    "plt.savefig(f\"Figures/{dataset_name}_assortativity.png\", dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2472fd6a",
   "metadata": {},
   "source": [
    "### Community structure\n",
    "\n",
    "The following block of code takes a filtering, computes the Laplacian, and performs spectral k-means clustering on it, according to the algorithm presented in \"Learning with Hypergraphs: Clustering, Classification, and Embedding\" by Zhou, Huang, and Schölkopf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07662290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the normalized Laplacian.\n",
    "def get_normalized_laplacian(H):\n",
    "    I = xgi.incidence_matrix(H)\n",
    "    present_nodes = np.sort(np.unique(I.nonzero()[0]))\n",
    "    I = I[present_nodes, :]\n",
    "    # A = I I^T\n",
    "    A = I.dot(I.T)\n",
    "    n = A.shape[0]\n",
    "    # zero the diagonal\n",
    "    A = A - diags(A.diagonal())\n",
    "    degrees = np.sum(A, axis=1)\n",
    "    d = []\n",
    "    for i in range(n):\n",
    "        d.append(degrees[i])\n",
    "\n",
    "    # Compute the D^(-1/2) degree matrix.\n",
    "    d = [1 / math.sqrt(degree) for degree in d]\n",
    "    D = diags(d)\n",
    "    A = D @ A @ D\n",
    "    In = diags([1.0 for i in range(n)])\n",
    "    L = 0.5 * (In - A)\n",
    "    return present_nodes, L\n",
    "\n",
    "\n",
    "# Perform k-means clustering on the eigenvectors of the\n",
    "# Laplacian of a hypergraph.\n",
    "def get_zhou_clusters(H, m):\n",
    "    try:\n",
    "        present_nodes, L = get_normalized_laplacian(H)\n",
    "        _, v = eigsh(L, k=m, which=\"SM\")\n",
    "        kmeans = KMeans(n_clusters=m).fit(v)\n",
    "        return present_nodes, kmeans.labels_\n",
    "    except:\n",
    "        return [], m * np.ones(H.num_nodes)\n",
    "\n",
    "\n",
    "# Get the community labels for each hypergraph in the filtering set.\n",
    "def get_communities(filtering, m):\n",
    "    K = len(filtering)\n",
    "    n = filtering[0].num_nodes\n",
    "    community_labels = np.zeros([K, n])\n",
    "    community_labels[:] = m\n",
    "    for i in range(K):\n",
    "        print(i)\n",
    "        present_nodes, labels = get_zhou_clusters(filtering[i], m)\n",
    "        for (j, node) in enumerate(present_nodes):\n",
    "            community_labels[i, node] = labels[j]\n",
    "\n",
    "    return community_labels\n",
    "\n",
    "\n",
    "# convert a vector of community labels to\n",
    "def get_clusters_from_label_vector(labels, m):\n",
    "    label_dict = {}\n",
    "    clusters = []\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] not in label_dict and labels[i] != m:\n",
    "            label_dict[labels[i]] = [i]\n",
    "        elif labels[i] != m:\n",
    "            label_dict[labels[i]].append(i)\n",
    "    for i in np.sort(list(label_dict.keys())):\n",
    "        clusters.append(label_dict[i])\n",
    "    return clusters\n",
    "\n",
    "\n",
    "# perform hungarian matching to permute the community labels\n",
    "# so that the overlap between community labels is as consistent\n",
    "# as possible.\n",
    "def hungarian_matching(c1, c2):\n",
    "    n = len(c1)\n",
    "    if len(c1) != len(c2):\n",
    "        print(\"clusterings are not of the same length!\")\n",
    "    C = []\n",
    "    for i in range(n):\n",
    "        C.append([])\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if len(set(c1[i]).union(set(c2[j]))) > 0:\n",
    "                set_diff_frac = (\n",
    "                    -0.5\n",
    "                    * (len(c1[i]) + len(c2[j]))\n",
    "                    * len(set(c1[i]).intersection(set(c2[j])))\n",
    "                    / len(set(c1[i]).union(set(c2[j])))\n",
    "                )\n",
    "            else:\n",
    "                set_diff_frac = 1000\n",
    "            cost = set_diff_frac\n",
    "            C[i].append(cost)\n",
    "    index_1, index_2 = linear_sum_assignment(C)\n",
    "    return index_1, index_2\n",
    "\n",
    "\n",
    "def get_val(d, key):\n",
    "    # to consistently relabel from 0\n",
    "    if key not in d:\n",
    "        n = len(d)\n",
    "        d[key] = n\n",
    "        return n\n",
    "    return d[key]\n",
    "\n",
    "\n",
    "# permute the community labels of an entire filtering set (using hungarian\n",
    "# matching) so that the community labels are roughly consistent across the filtering parameters.\n",
    "def match_communities(community_labels, m):\n",
    "    ## community_labels is K arrays of length n each, each encoding the community labels for each filtering\n",
    "    K = len(community_labels)\n",
    "    for i in range(K - 1):\n",
    "        current_labels = community_labels[i]\n",
    "        next_labels = community_labels[i + 1]\n",
    "        # get clusters of nodes (list of lists)\n",
    "        current_clusters = get_clusters_from_label_vector(current_labels, m)\n",
    "        next_clusters = get_clusters_from_label_vector(next_labels, m)\n",
    "        # rearrange labels of next_clusters to most closely line up with that of current clusters\n",
    "        old_index, new_index = hungarian_matching(current_clusters, next_clusters)\n",
    "        label_map = dict(zip(new_index, old_index))\n",
    "        for j in range(len(next_labels)):\n",
    "            # nodes with a label of m must not be assigned to another cluster since they correspond to missing nodes\n",
    "            if (next_labels[j] != m) & (current_labels[j] != m):\n",
    "                next_labels[j] = label_map[next_labels[j]]\n",
    "        community_labels[i + 1] = copy.deepcopy(next_labels)\n",
    "    first = copy.deepcopy(community_labels[0])\n",
    "    # print(np.unique(np.array(first)))\n",
    "    first_clusters = get_clusters_from_label_vector(first, m)\n",
    "    # print(len(first_clusters))\n",
    "    cluster_length = []\n",
    "    for (i, cluster) in enumerate(first_clusters):\n",
    "        cluster_length.append(len(cluster))\n",
    "    # get ranming of clusters by size\n",
    "    cluster_ranm = np.flip(np.argsort(cluster_length))\n",
    "    # print(cluster_ranm)\n",
    "    ranm_label_map = dict(zip(cluster_ranm, [i for i in range(len(cluster_ranm))]))\n",
    "    for (i, label) in enumerate(first):\n",
    "        try:\n",
    "            first[i] = ranm_label_map[int(label)]\n",
    "        except:\n",
    "            continue\n",
    "    index = np.argsort(first)\n",
    "    color_map = {}\n",
    "    for (i, c) in enumerate(community_labels):\n",
    "        community_labels[i] = c[index]\n",
    "        for (j, c_) in enumerate(c):\n",
    "            if community_labels[i][j] != m:\n",
    "                community_labels[i][j] = get_val(color_map, c_)\n",
    "    for i in range(len(community_labels)):\n",
    "        for j in range(len(community_labels[i])):\n",
    "            if community_labels[i][j] == m:\n",
    "                community_labels[i][j] = np.NaN\n",
    "    return community_labels, index\n",
    "\n",
    "\n",
    "# permute the community labels of an entire filtering set (using hungarian\n",
    "# matching) so that the community labels are roughly consistent across the filtering parameters,\n",
    "# based on the results of the GEQ filtering.\n",
    "def match_communities_node_reordering_on_geq(community_labels, m, index, c_geq):\n",
    "    ## community_labels is m arrays of length n each, each encoding the community labels for each filtering\n",
    "    K = len(community_labels)\n",
    "    for i in range(K - 1):\n",
    "        current_labels = community_labels[i]\n",
    "        next_labels = community_labels[i + 1]\n",
    "\n",
    "        # get clusters of nodes (list of lists)\n",
    "        current_clusters = get_clusters_from_label_vector(current_labels, m)\n",
    "        next_clusters = get_clusters_from_label_vector(next_labels, m)\n",
    "\n",
    "        # rearrange labels of next_clusters to most closely line up with that of current clusters\n",
    "        # this checks for the case where there are no communities\n",
    "        if len(current_clusters) == len(next_clusters):\n",
    "            old_index, new_index = hungarian_matching(current_clusters, next_clusters)\n",
    "            label_map = dict(zip(new_index, old_index))\n",
    "            for j in range(len(next_labels)):\n",
    "                # nodes with a label of k must not be assigned to another cluster since they correspond to missing nodes\n",
    "                if (next_labels[j] != m) & (current_labels[j] != m):\n",
    "                    next_labels[j] = label_map[next_labels[j]]\n",
    "            community_labels[i + 1] = copy.deepcopy(next_labels)\n",
    "\n",
    "    for (i, c) in enumerate(community_labels):\n",
    "        community_labels[i] = c[index]\n",
    "    first = copy.deepcopy(community_labels[0])\n",
    "    mode = stats.mode(first)[0]\n",
    "    for j, f in enumerate(first):\n",
    "        if f == m:\n",
    "            first[j] = mode\n",
    "    current_clusters = get_clusters_from_label_vector(first, m)\n",
    "\n",
    "    geq_clusters = get_clusters_from_label_vector(c_geq, m)\n",
    "\n",
    "    old_index, new_index = hungarian_matching(current_clusters, geq_clusters)\n",
    "    label_map = dict(zip(old_index, new_index))\n",
    "\n",
    "    for i in range(len(community_labels)):\n",
    "        for j in range(len(community_labels[i])):\n",
    "            if community_labels[i][j] == m:\n",
    "                community_labels[i][j] = np.NaN\n",
    "            else:\n",
    "                community_labels[i][j] = label_map[community_labels[i][j]]\n",
    "    return community_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b7ccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the community labels\n",
    "community_labels = dict()\n",
    "m = {\n",
    "    \"email-enron\": 5,\n",
    "    \"email-eu\": 42,\n",
    "    \"disgenenet\": 22,\n",
    "    \"diseasome\": 10,\n",
    "    \"contact-primary-school\": 10,\n",
    "    \"contact-high-school\": 9,\n",
    "}\n",
    "\n",
    "try:\n",
    "    with open(f\"Data/{dataset_name}_zhou_communities.json\") as file:\n",
    "        community_labels = json.loads(file.read())\n",
    "except:\n",
    "    _label = \"GEQ\"\n",
    "    print(_label)\n",
    "    f = filterings[_label]\n",
    "    c = get_communities(f, m[dataset_name])\n",
    "    c0 = c[0]\n",
    "\n",
    "    c, index_geq = match_communities(c, m[dataset_name])\n",
    "    community_labels[\"GEQ\"] = c.tolist()\n",
    "    all_comm = c[0]\n",
    "\n",
    "    for label in [\"uniform\", \"LEQ\", \"exclusion\"]:\n",
    "        print(label)\n",
    "        f = filterings[label]\n",
    "        c = get_communities(f, m[dataset_name])\n",
    "        c = match_communities_node_reordering_on_geq(c, m[dataset_name], index_geq, c0)\n",
    "        community_labels[label] = c.tolist()\n",
    "\n",
    "    datastring = json.dumps(community_labels, indent=2)\n",
    "    with open(f\"Data/{dataset_name}_zhou_communities.json\", \"w\") as output_file:\n",
    "        output_file.write(datastring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecfd17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the community labels for each of the four filterings.\n",
    "custom_palette = ListedColormap(sns.color_palette(\"plasma\", m[dataset_name]).as_hex())\n",
    "tick_fontsize = 14\n",
    "axis_label_fontsize = 18\n",
    "legend_fontsize = 16\n",
    "title_fontsize = 16\n",
    "xtick_skip = {\n",
    "    \"email-enron\": 3,\n",
    "    \"email-eu\": 10,\n",
    "    \"disgenenet\": 3,\n",
    "    \"diseasome\": 2,\n",
    "    \"contact-primary-school\": 1,\n",
    "    \"contact-high-school\": 1,\n",
    "}\n",
    "\n",
    "ytick_skip = {\n",
    "    \"email-enron\": 20,\n",
    "    \"email-eu\": 200,\n",
    "    \"disgenenet\": 300,\n",
    "    \"diseasome\": 100,\n",
    "    \"contact-primary-school\": 40,\n",
    "    \"contact-high-school\": 60,\n",
    "}\n",
    "colorbar_fontsize = 18\n",
    "colorbar_skip = {\n",
    "    \"email-enron\": 1,\n",
    "    \"email-eu\": 5,\n",
    "    \"disgenenet\": 3,\n",
    "    \"contact-primary-school\": 1,\n",
    "    \"contact-high-school\": 1,\n",
    "}\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "\n",
    "for i, label in enumerate(filterings):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.title(f\"{label} filtering\", fontsize=title_fontsize)\n",
    "    # plot the heatmap\n",
    "    im = plt.imshow(\n",
    "        np.flipud(np.array(community_labels[label]).T),\n",
    "        extent=(\n",
    "            min(filtering_parameters),\n",
    "            max(filtering_parameters),\n",
    "            0,\n",
    "            dataset.num_nodes,\n",
    "        ),\n",
    "        aspect=\"auto\",\n",
    "        interpolation=\"none\",\n",
    "        rasterized=True,\n",
    "        cmap=custom_palette,\n",
    "    )\n",
    "    plt.xticks(\n",
    "        filtering_parameters[:: xtick_skip[dataset_name]], fontsize=tick_fontsize\n",
    "    )\n",
    "    plt.yticks(nodes[:: ytick_skip[dataset_name]], fontsize=tick_fontsize)\n",
    "\n",
    "    if i == 2:\n",
    "        plt.xlabel(r\"$k$\", fontsize=axis_label_fontsize)\n",
    "        plt.ylabel(\"Node label\", fontsize=axis_label_fontsize)\n",
    "\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, left=0.1, right=0.8, wspace=0.3, hspace=0.3)\n",
    "cbar_ax = fig.add_axes([0.82, 0.15, 0.03, 0.8])\n",
    "cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "# xtick_skip = 1\n",
    "cbar.set_ticks([i for i in range(m[dataset_name])][:: colorbar_skip[dataset_name]])\n",
    "cbar.set_ticklabels(\n",
    "    [i + 1 for i in range(m[dataset_name])][:: colorbar_skip[dataset_name]]\n",
    ")\n",
    "cbar.ax.tick_params(labelsize=tick_fontsize, length=0)\n",
    "cbar.set_label(\n",
    "    r\"Community label\", fontsize=colorbar_fontsize, rotation=270, labelpad=25\n",
    ")\n",
    "\n",
    "plt.savefig(\n",
    "    f\"Figures/{dataset_name}_zhou_communities.pdf\", dpi=1000, bbox_inches=\"tight\"\n",
    ")\n",
    "plt.savefig(\n",
    "    f\"Figures/{dataset_name}_zhou_communities.png\", dpi=1000, bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd8f039",
   "metadata": {},
   "source": [
    "### Centrality\n",
    "\n",
    "The following code implements the betweenness centrality as defined in \"Hypergraph models of biological networks to identify genes critical to pathogenic viral response\" by Feng et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac27748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the betweenness centrality for every hypergraph in a filtering set.\n",
    "def get_centrality(filtering):\n",
    "    K = len(filtering)\n",
    "    n = filtering[0].num_nodes\n",
    "    c = np.zeros([K, n])\n",
    "    for i in range(K):\n",
    "        try:\n",
    "            hypergraph = filtering[i]\n",
    "            # compute the betweenness centrality of the unweighted pairwise projection\n",
    "            A = xgi.adjacency_matrix(hypergraph)\n",
    "            G = nx.from_scipy_sparse_array(A)\n",
    "            # we compute the betweenness centrality using NetworkX\n",
    "            betweenness_dict = nx.betweenness_centrality(G, weight=\"weight\")\n",
    "            betweenness = np.zeros(len(betweenness_dict))\n",
    "            for j in range(len(betweenness_dict)):\n",
    "                betweenness[j] = betweenness_dict[j]\n",
    "\n",
    "            # normalize with the infinity norm\n",
    "            maximum = max(betweenness)\n",
    "            if maximum > 0:\n",
    "                betweenness /= maximum\n",
    "            c[i] = betweenness\n",
    "        except:\n",
    "            c[i] = 0\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b51534",
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = dict()\n",
    "\n",
    "try:\n",
    "    with open(f\"Data/{dataset_name}_centrality.json\") as file:\n",
    "        bc = json.loads(file.read())\n",
    "except:\n",
    "    for label in filterings:\n",
    "        f = filterings[label]\n",
    "        c = get_centrality(f)\n",
    "        print(label)\n",
    "        for (i, c_) in enumerate(c):\n",
    "            print(i)\n",
    "            c[i] = c_[index_geq]\n",
    "        bc[label] = c.tolist()\n",
    "    datastring = json.dumps(bc, indent=2)\n",
    "    with open(f\"Data/{dataset_name}_centrality.json\", \"w\") as output_file:\n",
    "        output_file.write(datastring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7ffd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the betweenness centrality.\n",
    "tick_fontsize = 14\n",
    "axis_label_fontsize = 18\n",
    "legend_fontsize = 16\n",
    "title_fontsize = 16\n",
    "xtick_skip = {\n",
    "    \"email-enron\": 3,\n",
    "    \"email-eu\": 10,\n",
    "    \"disgenenet\": 3,\n",
    "    \"diseasome\": 2,\n",
    "    \"contact-primary-school\": 1,\n",
    "    \"contact-high-school\": 1,\n",
    "}\n",
    "ytick_skip = {\n",
    "    \"email-enron\": 20,\n",
    "    \"email-eu\": 200,\n",
    "    \"disgenenet\": 300,\n",
    "    \"diseasome\": 100,\n",
    "    \"contact-primary-school\": 40,\n",
    "    \"contact-high-school\": 60,\n",
    "}\n",
    "colorbar_fontsize = 18\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "for i, label in enumerate(filterings):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.title(f\"{label} filtering\", fontsize=title_fontsize)\n",
    "    # plot the heatmap\n",
    "    im = plt.imshow(\n",
    "        np.flipud(np.array(bc[label]).T),\n",
    "        extent=(\n",
    "            min(filtering_parameters),\n",
    "            max(filtering_parameters),\n",
    "            0,\n",
    "            dataset.num_nodes,\n",
    "        ),\n",
    "        aspect=\"auto\",\n",
    "        interpolation=\"none\",\n",
    "        rasterized=True,\n",
    "        cmap=\"plasma\",\n",
    "    )\n",
    "    plt.xticks(\n",
    "        filtering_parameters[:: xtick_skip[dataset_name]], fontsize=tick_fontsize\n",
    "    )\n",
    "    plt.yticks(nodes[:: ytick_skip[dataset_name]], fontsize=tick_fontsize)\n",
    "\n",
    "    if i == 2:\n",
    "        plt.xlabel(r\"$k$\", fontsize=axis_label_fontsize)\n",
    "        plt.ylabel(\"Node label\", fontsize=axis_label_fontsize)\n",
    "\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, left=0.1, right=0.8, wspace=0.3, hspace=0.3)\n",
    "cbar_ax = fig.add_axes([0.82, 0.15, 0.03, 0.8])\n",
    "cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "cbar.ax.tick_params(labelsize=tick_fontsize)\n",
    "cbar.set_label(r\"$BC(n)$\", fontsize=colorbar_fontsize, rotation=270, labelpad=15)\n",
    "\n",
    "plt.savefig(f\"Figures/{dataset_name}_centrality.pdf\", bbox_inches=\"tight\")\n",
    "plt.savefig(f\"Figures/{dataset_name}_centrality.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6619cd59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
